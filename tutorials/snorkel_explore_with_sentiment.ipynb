{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `snorkel` on `sentiment` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.     -1\n",
       "2          Not tasty and the texture was just nasty.     -1\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"yelp_labelled.txt\"\n",
    "data = pd.read_csv(DATA_PATH, header=None, sep=\"\\t\")\n",
    "print(\"data shape\", data.shape)\n",
    "data = data.rename(columns={0: \"text\", 1: \"label\"})\n",
    "data[\"label\"] = data[\"label\"].apply(lambda x: -1 if x == 0 else x)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing duplicates\n",
    "\n",
    "`SQLLite` doesn't accept duplicate documents...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>I love this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>I love this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  label\n",
       "334  I love this place.      1\n",
       "814  I love this place.      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"text\"].str.contains(\"love this place\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>I love this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  label\n",
       "334  I love this place.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset=[\"text\"])\n",
    "data.to_csv(\"yelp_labelled_NO_DUPS.txt\", index=False, sep=\"\\t\")\n",
    "data_no_dups = pd.read_csv(\"yelp_labelled_NO_DUPS.txt\", sep=\"\\t\")\n",
    "data_no_dups[data_no_dups[\"text\"].str.contains(\"love this place\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `spaCy` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "p = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_dups[\"doc\"] = data_no_dups[\"text\"].apply(lambda x: p(x))\n",
    "data_no_dups[\"tokens\"] = data_no_dups[\"doc\"].apply(lambda x: [t for t in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>doc</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>(Wow, ..., Loved, this, place, .)</td>\n",
       "      <td>[Wow, ..., Loved, this, place, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>-1</td>\n",
       "      <td>(Crust, is, not, good, .)</td>\n",
       "      <td>[Crust, is, not, good, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>-1</td>\n",
       "      <td>(Not, tasty, and, the, texture, was, just, nas...</td>\n",
       "      <td>[Not, tasty, and, the, texture, was, just, nas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Stopped, by, during, the, late, May, bank, ho...</td>\n",
       "      <td>[Stopped, by, during, the, late, May, bank, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>(The, selection, on, the, menu, was, great, an...</td>\n",
       "      <td>[The, selection, on, the, menu, was, great, an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0                           Wow... Loved this place.      1   \n",
       "1                                 Crust is not good.     -1   \n",
       "2          Not tasty and the texture was just nasty.     -1   \n",
       "3  Stopped by during the late May bank holiday of...      1   \n",
       "4  The selection on the menu was great and so wer...      1   \n",
       "\n",
       "                                                 doc  \\\n",
       "0                  (Wow, ..., Loved, this, place, .)   \n",
       "1                          (Crust, is, not, good, .)   \n",
       "2  (Not, tasty, and, the, texture, was, just, nas...   \n",
       "3  (Stopped, by, during, the, late, May, bank, ho...   \n",
       "4  (The, selection, on, the, menu, was, great, an...   \n",
       "\n",
       "                                              tokens  \n",
       "0                  [Wow, ..., Loved, this, place, .]  \n",
       "1                          [Crust, is, not, good, .]  \n",
       "2  [Not, tasty, and, the, texture, was, just, nas...  \n",
       "3  [Stopped, by, during, the, late, May, bank, ho...  \n",
       "4  [The, selection, on, the, menu, was, great, an...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_dups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(tokens):\n",
    "    return set([t.lemma_ for t in tokens])\n",
    "\n",
    "pos_words = set([\n",
    "    \"good\",\n",
    "#     \"awesome\",\n",
    "#     \"great\",\n",
    "#     \"delicious\",\n",
    "#     \"wonderful\",\n",
    "#     \"fast\"\n",
    "])\n",
    "\n",
    "pos_tokens = [t for t in p(\" \".join(pos_words))]\n",
    "\n",
    "neg_words = set([\n",
    "    \"bad\",\n",
    "#     \"terrible\",\n",
    "#     \"horrible\",\n",
    "#     \"unplesant\",\n",
    "#     \"suck\",\n",
    "#     \"slow\"\n",
    "])\n",
    "\n",
    "neg_tokens = [t for t in p(\" \".join(neg_words))]\n",
    "\n",
    "strict_thresh = 0.8\n",
    "low_thresh = 0.6\n",
    "word_thresh = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_positive_word(tokens):\n",
    "    words_in_doc = get_words(tokens)\n",
    "    for w in words_in_doc:\n",
    "        if w in pos_words:\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "def LF_negative_word(tokens):\n",
    "    words_in_doc = get_words(tokens)\n",
    "    for w in words_in_doc:\n",
    "        if w in neg_words:\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "def LF_positive_word_sim_strict(tokens):\n",
    "    for w in tokens:\n",
    "        for ww in pos_tokens:\n",
    "            if w.similarity(ww) >= strict_thresh:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def LF_negative_word_sim_strict(tokens):\n",
    "    for w in tokens:\n",
    "        for ww in neg_tokens:\n",
    "            if w.similarity(ww) >= strict_thresh:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def LF_positive_word_sim_loose(tokens):\n",
    "    for w in tokens:\n",
    "        for ww in pos_tokens:\n",
    "            if w.similarity(ww) >= low_thresh:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def LF_negative_word_sim_loose(tokens):\n",
    "    for w in tokens:\n",
    "        for ww in neg_tokens:\n",
    "            if w.similarity(ww) >= low_thresh:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def LF_not(tokens):\n",
    "    if \"not\" in get_words(tokens):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def LF_exclamation(tokens):\n",
    "    if \"!\" in get_words(tokens):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def LF_review_length(tokens):\n",
    "    if len(tokens) <= word_thresh:\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_fns = [\n",
    "    LF_positive_word, \n",
    "    LF_negative_word, \n",
    "    LF_positive_word_sim_strict, \n",
    "    LF_negative_word_sim_strict, \n",
    "    LF_positive_word_sim_loose, \n",
    "    LF_negative_word_sim_loose, \n",
    "    LF_not,\n",
    "    LF_exclamation,\n",
    "    LF_review_length\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "L = np.zeros(\n",
    "    (len(L_fns), data_no_dups.shape[0])\n",
    "    ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  1,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  1,  1, ...,  1,  0,  1],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [-1, -1,  0, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, lf in enumerate(L_fns):\n",
    "    for j, row in data_no_dups.iterrows():\n",
    "        L[i, j] = lf(row[\"tokens\"])\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_table = np.zeros((len(L), 2))\n",
    "for i in range(len(L)):\n",
    "    # coverage\n",
    "    stats_table[i,0] = np.sum(L[i,:] != 0)/data_no_dups.shape[0]\n",
    "    # accuracy\n",
    "    stats_table[i,1] = np.sum(L[i,:] == data_no_dups[\"label\"].values)/float(np.sum(L[i,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_positive_word</th>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_negative_word</th>\n",
       "      <td>0.032129</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_positive_word_sim_strict</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.801205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_negative_word_sim_strict</th>\n",
       "      <td>0.017068</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_positive_word_sim_loose</th>\n",
       "      <td>0.803213</td>\n",
       "      <td>0.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_negative_word_sim_loose</th>\n",
       "      <td>0.706827</td>\n",
       "      <td>0.434659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_not</th>\n",
       "      <td>0.203815</td>\n",
       "      <td>0.187192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_exclamation</th>\n",
       "      <td>0.165663</td>\n",
       "      <td>0.672727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_review_length</th>\n",
       "      <td>0.205823</td>\n",
       "      <td>0.463415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Coverage  Accuracy\n",
       "LF_positive_word             0.120482  0.716667\n",
       "LF_negative_word             0.032129  0.968750\n",
       "LF_positive_word_sim_strict  0.166667  0.801205\n",
       "LF_negative_word_sim_strict  0.017068  0.000000\n",
       "LF_positive_word_sim_loose   0.803213  0.505000\n",
       "LF_negative_word_sim_loose   0.706827  0.434659\n",
       "LF_not                       0.203815  0.187192\n",
       "LF_exclamation               0.165663  0.672727\n",
       "LF_review_length             0.205823  0.463415"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_table = pd.DataFrame(stats_table, index = [lf.__name__ for lf in L_fns], columns = [\"Coverage\", \"Accuracy\"])\n",
    "stats_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at unlabeled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cocktails are all handmade and delicious.\n",
      "The turkey and roast beef were bland.\n",
      "In summary, this was a largely disappointing dining experience.\n",
      "I LOVED their mussels cooked in this wine reduction, the duck was tender, and their potato dishes were delicious.\n",
      "Will go back next trip out.\n",
      "Phenomenal food, service and ambiance.\n",
      "We waited for forty five minutes in vain.\n",
      "Their menu is diverse, and reasonably priced.\n",
      "The atmosphere is modern and hip, while maintaining a touch of coziness.\n",
      "I ordered the Lemon raspberry ice cocktail which was also incredible.\n",
      "The live music on Fridays totally blows.\n",
      "I've lived here since 1979 and this was the first (and last) time I've stepped foot into this place.\n",
      "Must have been an off night at this place.\n",
      "The sides are delish - mixed mushrooms, yukon gold puree, white corn - beateous.\n",
      "My friend loved the salmon tartar.\n",
      "The service was a bit lacking.\n",
      "I loved the bacon wrapped dates.\n",
      "As for the \"mains,\" also uninspired.\n",
      "The seafood was fresh and generous in portion.\n",
      "For a self proclaimed coffee cafe, I was wildly disappointed.\n",
      "I liked the patio and the service was outstanding.\n",
      "Please stay away from the shrimp stir fried noodles.\n",
      "The only downside is the service.\n",
      "I have eaten here multiple times, and each time the food was delicious.\n",
      "We sat another ten minutes and finally gave up and left.\n",
      "Sadly, Gordon Ramsey's Steak is a place we shall sharply avoid during our next trip to Vegas.\n",
      "My wife had the Lobster Bisque soup which was lukewarm.\n",
      "I will come back here every time I'm in Vegas.\n",
      "Def coming back to bowl next time\n",
      "Cooked to perfection and the service was impeccable.\n",
      "Strike 2, who wants to be rushed.\n",
      "Ordered an appetizer and took 40 minutes and then the pizza another 10 minutes.\n",
      "I'd love to go back.\n",
      "Im in AZ all the time and now have my new spot.\n",
      "Third, the cheese on my friend's burger was cold.\n",
      "We enjoy their pizza and brunch.\n",
      "I will be back many times soon.\n",
      "Their monster chicken fried steak and eggs is my all time favorite.\n",
      "The yellowtail carpaccio was melt in your mouth fresh.\n",
      "The desserts were a bit strange.\n",
      "I *heart* this place.\n",
      "My salad had a bland vinegrette on the baby greens and hearts of Palm.\n",
      "Eew... This location needs a complete overhaul.\n",
      "After I pulled up my car I waited for another 15 minutes before being acknowledged.\n",
      "The croutons also taste homemade which is an extra plus.\n",
      "High-quality chicken on the chicken Caesar salad.\n",
      "We were promptly greeted and seated.\n",
      "The owners are super friendly and the staff is courteous.\n",
      "Google mediocre and I imagine Smashburger will pop up.\n",
      "As a sushi lover avoid this place by all means.\n",
      "Went for lunch - service was slow.\n",
      "Nicest Chinese restaurant I've been in a while.\n",
      "I love the decor with the Chinese calligraphy wall paper.\n",
      "An extensive menu provides lots of options for breakfast.\n",
      "The chains, which I'm no fan of, beat this place easily.\n",
      "The seasonal fruit was fresh white peach puree.\n",
      "This place should honestly be blown up.\n",
      "The crÃªpe was delicate and thin and moist.\n",
      "Do yourself a favor and stay away from this dish.\n",
      "I love their fries and their beans.\n",
      "For sushi on the Strip, this is the place to go.\n",
      "My boyfriend and i sat at the bar and had a completely delightful experience.\n",
      "I found a six inch long piece of wire in my salsa.\n",
      "My boyfriend tried the Mediterranean Chicken Salad and fell in love.\n",
      "Their rotating beers on tap is also a highlight of this place.\n",
      "Pricing is a bit of a concern at Mellow Mushroom.\n",
      "After the disappointing dinner we went elsewhere for dessert.\n",
      "This is my new fav Vegas buffet spot.\n",
      "Every time I eat here, I see caring teamwork to a professional degree.\n",
      "The RI style calamari was a joke.\n",
      "WAAAAAAyyyyyyyyyy over rated is all I am saying.\n",
      "The cashew cream sauce was bland and the vegetables were undercooked.\n",
      "The chipolte ranch dipping sause was tasteless, seemed thin and watered down with no heat.\n",
      "Then our food came out, disappointment ensued.\n",
      "The real disappointment was our waiter.\n",
      "Your servers suck, wait, correction, our server Heimer sucked.\n",
      "I vomited in the bathroom mid lunch.\n",
      "All in all, Ha Long Bay was a bit of a flop.\n",
      "A lady at the table next to us found a live green caterpillar In her salad.\n"
     ]
    }
   ],
   "source": [
    "for i in range(L.shape[1]):\n",
    "    if not 1 in L[:,i] and not -1 in L[:,i]:\n",
    "        print(data_no_dups.loc[i, \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "L_train = sparse.csr_matrix(L.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of Majority Vote on Train Set:  0.9206827309236948\n",
      "Accuracy of Majority Vote on Train Set:  0.4397590361445783\n"
     ]
    }
   ],
   "source": [
    "mv_labels = np.sign(np.sum(L.T,1))\n",
    "print ('Coverage of Majority Vote on Train Set: ', np.sum(np.sign(np.sum(np.abs(L.T),1)) != 0)/float(data_no_dups.shape[0]))\n",
    "print ('Accuracy of Majority Vote on Train Set: ', np.mean(mv_labels == data_no_dups[\"label\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `snorkel` Generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n"
     ]
    }
   ],
   "source": [
    "gen_model = GenerativeModel()\n",
    "\n",
    "gen_model.train(L.T, epochs=100, decay=0.95, step_size= 0.01/ L.shape[1], reg_param=1e-6)\n",
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_positive_word</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.843874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_negative_word</th>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.829018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_positive_word_sim_strict</th>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.835431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_negative_word_sim_strict</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.829664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_positive_word_sim_loose</th>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.898254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_negative_word_sim_loose</th>\n",
       "      <td>0.434659</td>\n",
       "      <td>0.887147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_not</th>\n",
       "      <td>0.187192</td>\n",
       "      <td>0.849095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_exclamation</th>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.847506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_review_length</th>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.817487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Empirical Acc.  Learned Acc.\n",
       "LF_positive_word                   0.716667      0.843874\n",
       "LF_negative_word                   0.968750      0.829018\n",
       "LF_positive_word_sim_strict        0.801205      0.835431\n",
       "LF_negative_word_sim_strict        0.000000      0.829664\n",
       "LF_positive_word_sim_loose         0.505000      0.898254\n",
       "LF_negative_word_sim_loose         0.434659      0.887147\n",
       "LF_not                             0.187192      0.849095\n",
       "LF_exclamation                     0.672727      0.847506\n",
       "LF_review_length                   0.463415      0.817487"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_table = gen_model.learned_lf_stats()\n",
    "empirical_acc = stats_table.values[:,1]\n",
    "learned_acc = learned_table.values[:,0]\n",
    "compared_stats = pd.DataFrame(np.stack((empirical_acc, learned_acc)).T,\n",
    "                             index = [lf.__name__ for lf in L_fns],\n",
    "                             columns=['Empirical Acc.', 'Learned Acc.'])\n",
    "\n",
    "compared_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of Generative Model on Train Set: 0.9206827309236948\n",
      "Accuracy of Generative Model on Train Set: 0.5020080321285141\n"
     ]
    }
   ],
   "source": [
    "labels = 2 * (train_marginals >= 0.8) - 1\n",
    "print ('Coverage of Generative Model on Train Set:', np.sum(train_marginals != 0.5)/float(len(train_marginals)))\n",
    "print ('Accuracy of Generative Model on Train Set:', np.mean(labels == data_no_dups[\"label\"].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
